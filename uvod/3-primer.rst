.. _primer:

Пример конструкције функције губитка ФПНМ-а
=============================================

Резоновање из претходне секције :ref:`metoda` ћемо поткрепити једноставним примером. Започнимо прво тако што ћемо узети неку једноставну диференцијалну једначину. Изабраћемо `логистичку једначину <https://en.wikipedia.org/wiki/Logistic_function>`_ (*logistic equation*) која представља модел раста популације. Та једначина гласи:

.. math::
    :label: eq:logisticka1

    \frac{df}{dt} = R \cdot t(1-t)

Функција *f(t)* представља стопу раста популације током времена *t*, док параметар *R* даје максималну стопу раста. Да бисмо од фамилије кривих које задовољавају решење ове обичне диференцијалне једначине изабрали једну криву као решење, морамо поставити гранични, тј. почетни услов. Нека то буде:

.. math::
    :label: eq:logisticka2

    f(t=0) = \frac12

Како је аналитичко решење ове једначине познато, поређењем са њим можемо проверити тачност ФПНМ методе. Тиме ћемо демонстрирати да су наведене технике примењиве и на комплексније обичне и парцијалне диференцијалне једначине. Да се подсетимо, ФПНМ су засноване на две фундаменталне особине неуронских мрежа:

* Формално је показано да су **неуронске мреже универзалне функције апроксимације**, тако да неуронска мрежа може да апроксимира било коју функцију, а самим тим и решење за нашу логистичку једначину.
* Једноставно је и јефтино израчунати изводе било ког реда излаза из неуронске мреже за било који дати улаз коришћењем **аутоматске диференцијације**. 

Дакле, као што је речено, можемо да конструишемо функцију губитка тако да, када се минимизује, диференцијална једначине буде задовољена:

.. math::
    :label: eq:logisticka3

    \mathcal{L}_{r} = \frac{df_{NN}(t)}{dt} - R \cdot t(1-t) = 0 

где је :math:`f_{NN}(t)` излаз неуронске мреже са једним улазом чији се извод израчунава аутоматским диференцирањем. Одмах видимо да, уколико је излаз из  неуронске мреже задовољава горњу једначину, заправо се та једначина решава. Да би се израчунао стварни допринос функцији губитка који се добија из диференцијалне једначине, потребно је специфицирати скуп тзв. колокационих тачака у домену проблема и проценити средњу квадратну грешку (*MSE*) или неку другу функцију губитка: 

.. math::
    :label: eq:logisticka4

    \mathcal{L}_{r} = \frac{1}{N_{r}}\sum_{i = 1}^{N_{r}} \left(  \left. \frac{df_{NN}}{dt} \right|_{t_i} - R t_j (1-t_i) \right)^2, 

где je :math:`N_{r}` број колокационих тачака :math:`t_j` у којима се рачуна функција губитка. Губитак заснован само на резидуалима не осигурава јединствено решење, па стога укључујемо и гранични услов тако што га додајемо функцији губитка на исти начин као у једначини :math:numref:`eq:logisticka4`:

.. math::
    :label: eq:logisticka5

    \mathcal{L}_{0} = \frac{1}{N_{0}}\sum_{i = 1}^{N_{0}} \left(  \left. f_{NN}(t) \right|_{t_i} - \frac12 \right)^2, \qquad t_i \approx 0

Сада имамо оба елемента да дефинишемо укупну функцију губитка :math:`\mathcal{L}`: 

.. math::
    :label: eq:logisticka6

    \mathcal{L} = \mathcal{L}_{r} + \mathcal{L}_{0}.

Током оптимизације (тренинга), горњи израз се минимизује и излаз из неуронска мрежа тренира да задовољи диференцијалну једначину и дат гранични услов, чиме се  апроксимира коначно решење диференцијалне једначине. ФПНМ је веома једноставан, и користећи идеју описану у претходном тексту, можемо додати више граничних услова, додати комплексније или решавати временски зависне вишедимензионалне проблеме користећи неуронску мрежу са вишеструким улазима. Решење једначине је добро позната сигмоид функција приказана на :numref:`sigmoid`.

.. _sigmoid:

.. figure:: logistic.png
    :width: 50%

    Сигмоидна функција која се добија као решење једначине :math:numref:`eq:logisticka1` са почетним условом :math:numref:`eq:logisticka2`.

Излагање ћемо наставити конкретним примерима са теоријском позадином, припадајућим програмским кодом који имплементира ФПНМ, као и анализом тачности и ефикасности ФПНМ решења у односу на аналитичко решење или решење добијено класичним нумеричким методама. 
