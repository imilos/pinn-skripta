.. _istorija:

Кратка историја развоја методе
================================

Последњих деценија се убрзано развијају различите методе дубоког учења решавање разних врста проблема, као што су препознавање слике, препознавање говора, обрада природног језика (*Natural Language Processing* - NLP), претраживање, системи за препоруке, биоинформатика итд. Међутим, традиционално надгледано дубоко учење није погодно за решавање баш свих врста проблема, без обзира на довољну количину доступних података који описују понашање моделованог система. Конкретно, проблеми описани кроз линеарне и нелинеарне једначине и системи једначина нису били у фокусу дубоког учења. Класичне нумеричке методе и даље држе апсолутни примат у нумеричком решавању парцијалних диференцијалних једначина. Методе као што су коначне разлике (*Finite Difference Method*), коначне запремине (*Finite Volume Method*), и коначни елементи (*Finite Element Method*) се и даље сматрају најсавременијим методама због њихове ефикасности и могућности примене у широком спектру области.

Међутим, решавање инверзних проблема класичним нумеричким методама захтева прорачуне који су изузетно рачунарски и временски захтевни, јер претрага за непознатим параметрима модела укључује итеративни поступак. Због тога се тачност често жртвује за ефикасност. Ове врсте претрага често нису исцрпне и укључују тзв. мета-хеуристике, без гаранције да ће се пронаћи најбоље (оптимално) решење. Примери из праксе аутора најчешће користе генетске алгоритме (*Genetic Algorithm* - GA), спроводе оптимизацију базирану на симулацији (*Simulation Based Optimization*) и захтевају употребу више стотина процесора да би се до решења које није гарантовано оптимално дошло у иоле разумном временском року, као што су показали :cite:t:`ivanovic2015elastic`, :cite:t:`simic2019optimizing` и :cite:t:`ivanovic2022efficient`. Поред кардиналоних потешкоћа при решавању инверзних проблема, ту су још и следећи недостаци класичних нумеричких метода:

* Није могуће на лак начин у модел укључити **податке са шумом**.
* **Генерисање прорачунске мреже** остаје сложена мануелна операција, често подложна грешкама. 
* Проблеме који укључују **већи број димензија** није могуће решити у реалним временским оквирима рачунарским ресурсима којима тренутно располажемо. 

Да би се елиминисали ови недостаци нумеричких метода, развијена је нова метода дубоког учења за решавање парцијалних диференцијалних једначина. Та методa, под именом **Физички засноване неуронске мреже** (*Physics Informed Neural Netorks* - PINN), користи се за решавање проблема надгледаног учења уз поштовање било ког закона физике описаног општим нелинеарним парцијалним диференцијалним једначинама :cite:t:`raissi2019physics`.

Главна иновација PINN-ова у поређењу са класичним неуронским мрежама је увођење функције губитка која кодира основне једначине физике, узима излаз дубоке мреже, која се зове апроксиматор, и израчунава а вредност губитка :cite:t:`markidis2021old`. Функција губитка која се односи на  диференцијалну једначину се минимизира обуком апроксиматорске неуронске мреже, где се диференцијални оператори примењују коришћењем аутоматске диференцијације. Аутоматска диференцијација је предуслов да би се уопште обавило тренирање пропагацијом уназад и поседују је све библиотеке за машинско учење, као што су *Tensorflow*, *PyTorch*, *Theano*, и друге.
