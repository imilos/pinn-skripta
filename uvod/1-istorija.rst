.. _istorija:

Кратка историја методе
================================

Током последње деценије се убрзано развијају различите методе дубоког учења решавање различитих врста проблема из области машинског учења, као што су препознавање слике, препознавање говора, обрада природног језика (*Natural Language Processing* - NLP), претраживање, системи за препоруке, биоинформатика, итд. Међутим, класично надгледано дубоко учење као метода није погодно за решавање баш свих врста проблема, без обзира на довољну количину доступних података који описују понашање моделованог система. На пример, проблеми описани кроз линеарне и нелинеарне једначине и системе једначина никад и нису били у фокусу дубоког учења. Класичне нумеричке методе и даље држе апсолутни примат у нумеричком решавању парцијалних диференцијалних једначина. Методе као што су коначне разлике (*Finite Difference Method*), коначне запремине (*Finite Volume Method*) и коначни елементи (*Finite Element Method*) се и даље сматрају најсавременијим методама због њихове робусности, ефикасности и могућности примене у широком спектру проблема.

Са друге стране, решавање нелинеарних инверзних проблема класичним нумеричким методама захтева прорачуне који су изузетно рачунарски и временски захтевни. Претрага оптималних параметра модела укључује итеративни поступак који често укључује десетине и стотине пролаза директне симулације. Због тога се тачност често жртвује за ефикасност, те ове врсте претрага често нису исцрпне и укључују тзв. мета-хеуристике, без гаранције да ће се пронаћи најбоље (оптимално) решење. Примери из праксе аутора и сарадника најчешће користе генетске алгоритме (*Genetic Algorithm* - GA), спроводе оптимизацију базирану на симулацији (*Simulation Based Optimization*) и захтевају употребу више стотина процесора да би се до решења (које није гарантовано оптимално) дошло у иоле разумном временском року, према :cite:t:`ivanovic2015elastic`, :cite:t:`simic2019optimizing` и :cite:t:`ivanovic2022efficient`. Поред кардиналоних потешкоћа при решавању инверзних проблема, очигледни су још и следећи недостаци класичних нумеричких метода:

- Није могуће на лак начин у модел укључити **податке са шумом**.
- **Генерисање прорачунске мреже** остаје сложена мануелна операција, често подложна грешкама. 
- Проблеме који укључују **већи број димензија** није могуће решити у реалним временским оквирима рачунарским ресурсима којима тренутно располажемо. 

Да би се елиминисали ови недостаци класичних нумеричких метода, развијена је нова метода дубоког учења за решавање парцијалних диференцијалних једначина. Та методa, под именом **Физички поткрепљене неуронске мреже** (*Physics Informed Neural Netorks* - PINN), користи се за решавање проблема надгледаног учења уз поштовање било ког закона физике описаног општим нелинеарним парцијалним диференцијалним једначинама :cite:t:`raissi2019physics`.

Главна иновација Фузички поткрепљених неуронских мрежа (ФПНМ) у поређењу са класичним дубоким неуронским мрежама је увођење функције губитка која кодира основне једначине физичких закона, узима излаз дубоке мреже, која се зове апроксиматор, и израчунава вредност губитка :cite:t:`markidis2021old`. Функција губитка која се односи на  диференцијалну једначину се минимизира обуком апроксиматорске неуронске мреже, где се диференцијални оператори примењују коришћењем аутоматске диференцијације. Аутоматска диференцијација је предуслов да би се уопште обавило тренирање пропагацијом уназад и поседују је све библиотеке за машинско учење, као што су *Tensorflow*, *PyTorch*, *Theano*, и друге.
